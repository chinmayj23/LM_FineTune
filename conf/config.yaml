# conf/config.yaml
model:
  name: "HuggingFaceTB/SmolLM-1.7B-Instruct"

data:
  book_path: "https://www.gutenberg.org/cache/epub/69087/pg69087.txt"
  chapter_start_pattern: "CHAPTER I"
  chapter_end_pattern: "CHAPTER II"
  instruction_dataset: "LNTANOooo/open_hermes2.5_v3"
  eval_samples: 20
  mmlu_samples: 20
  excerpt_size: 2048

training:
  learning_rate: 2e-5
  batch_size: 4
  num_epochs: 2
  max_length: 2048
  gradient_accumulation_steps: 4
  optimizer: AdamW
  weight_decay: 0.01

output:
  dir: "./output_lm_finetune"
